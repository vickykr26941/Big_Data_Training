{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.8/site-packages (1.1.3)\n",
      "Requirement already satisfied: numpy>=1.15.4 in ./anaconda3/lib/python3.8/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./anaconda3/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in ./anaconda3/lib/python3.8/site-packages (3.2.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.2 in ./anaconda3/lib/python3.8/site-packages (from pyspark) (0.10.9.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vicky</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rakesh</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rajeev</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shubham</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>umesh</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Name  Age\n",
       "0   vicky    21\n",
       "1   rakesh   22\n",
       "2   rajeev   23\n",
       "3  shubham   19\n",
       "4    umesh    3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/vkkr125/programming/bigdata/test.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in ./.local/lib/python3.8/site-packages (1.4.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('practice2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sparkContext.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2\n",
      "2 1\n",
      "3 2\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "lines = spark.sparkContext.textFile('/home/vkkr125/programming/bigdata/u.data')\n",
    "ratings = lines.map(lambda x : x.split()[2])\n",
    "result = ratings.countByValue()\n",
    "\n",
    "sorted_data = collections.OrderedDict(sorted(result.items()))\n",
    "for key,value in sorted_data.items():\n",
    "  print(key,value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sparkdataframe, function and manupulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.43.248:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa94e9d6c70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|    _c0|_c1|\n",
      "+-------+---+\n",
      "|   Name|Age|\n",
      "| vicky | 21|\n",
      "| rakesh| 22|\n",
      "| rajeev| 23|\n",
      "|shubham| 19|\n",
      "|  umesh|  3|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('/home/vkkr125/programming/bigdata/test.csv')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "| vicky | 21|\n",
      "| rakesh| 22|\n",
      "| rajeev| 23|\n",
      "|shubham| 19|\n",
      "|  umesh|  3|\n",
      "+-------+---+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the data with first row as column\n",
    "df_pyspark = spark.read.option('header','true').csv('/home/vkkr125/programming/bigdata/test.csv')\n",
    "df_pyspark.show()\n",
    "type(df_pyspark)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st tutorial"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# pyspark dataframe tutorial 1....................................\n",
    "1 reading the dataset \n",
    "2 checking the datatypes of column schema\n",
    "3 selecting columns and indexing \n",
    "4 check describe option simmiller to pandas\n",
    "5 adding columns\n",
    "6 droping columns\n",
    "7 Renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+\n",
      "|    Name|Age|Expricnce|\n",
      "+--------+---+---------+\n",
      "|   vicky| 21|        1|\n",
      "|  rajeev| 22|        3|\n",
      "|   mohan| 21|        4|\n",
      "|    modi|  3|        5|\n",
      "| shubham|  5|        8|\n",
      "|rajeever|  4|       44|\n",
      "|  preeti| 21|        2|\n",
      "+--------+---+---------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Expricnce: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('/home/vkkr125/programming/bigdata/test2.csv')\n",
    "df_pyspark.show()\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+\n",
      "|    Name|Age|Expricnce|\n",
      "+--------+---+---------+\n",
      "|   vicky| 21|        1|\n",
      "|  rajeev| 22|        3|\n",
      "|   mohan| 21|        4|\n",
      "|    modi|  3|        5|\n",
      "| shubham|  5|        8|\n",
      "|rajeever|  4|       44|\n",
      "|  preeti| 21|        2|\n",
      "+--------+---+---------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Expricnce: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark read the datatypes as string by default even that wast string or int or any datatype \n",
    "# we can provide inferschema to take as it is\n",
    "\n",
    "\n",
    "df_pyspark = spark.read.option('header','true').csv('/home/vkkr125/programming/bigdata/test2.csv',inferSchema = True)\n",
    "df_pyspark.show()\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+\n",
      "|    Name|Age|Expricnce|\n",
      "+--------+---+---------+\n",
      "|   vicky| 21|        1|\n",
      "|  rajeev| 22|        3|\n",
      "|   mohan| 21|        4|\n",
      "|    modi|  3|        5|\n",
      "| shubham|  5|        8|\n",
      "|rajeever|  4|       44|\n",
      "|  preeti| 21|        2|\n",
      "+--------+---+---------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Expricnce: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same as above \n",
    "df_pyspark = spark.read.csv('/home/vkkr125/programming/bigdata/test2.csv',header = True,inferSchema = True)\n",
    "df_pyspark.show()\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'Age', 'Expricnce']\n",
      "+-------+-----+------------------+------------------+\n",
      "|summary| Name|               Age|         Expricnce|\n",
      "+-------+-----+------------------+------------------+\n",
      "|  count|    7|                 7|                 7|\n",
      "|   mean| null|13.857142857142858| 9.571428571428571|\n",
      "| stddev| null| 9.245333443212877|15.349887544182451|\n",
      "|    min| modi|                 3|                 1|\n",
      "|    max|vicky|                22|                44|\n",
      "+-------+-----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df_pyspark.columns)\n",
    "df_pyspark.describe().show()\n",
    "# type(df_pyspark.columns[0])\n",
    "\n",
    "# here describe take string in computation as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='vicky', Age=21, Expricnce=1),\n",
       " Row(Name='rajeev', Age=22, Expricnce=3),\n",
       " Row(Name='mohan', Age=21, Expricnce=4),\n",
       " Row(Name='modi', Age=3, Expricnce=5)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Name|\n",
      "+--------+\n",
      "|   vicky|\n",
      "|  rajeev|\n",
      "|   mohan|\n",
      "|    modi|\n",
      "| shubham|\n",
      "|rajeever|\n",
      "|  preeti|\n",
      "+--------+\n",
      "\n",
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "|   vicky| 21|\n",
      "|  rajeev| 22|\n",
      "|   mohan| 21|\n",
      "|    modi|  3|\n",
      "| shubham|  5|\n",
      "|rajeever|  4|\n",
      "|  preeti| 21|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+---------+\n",
      "|    Name|Age|Expricnce|\n",
      "+--------+---+---------+\n",
      "|   vicky| 21|        1|\n",
      "|  rajeev| 22|        3|\n",
      "|   mohan| 21|        4|\n",
      "|    modi|  3|        5|\n",
      "| shubham|  5|        8|\n",
      "|rajeever|  4|       44|\n",
      "|  preeti| 21|        2|\n",
      "+--------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecting columns \n",
    "df_pyspark.select('Name').show()\n",
    "df_pyspark.select(['Name','Age']).show()\n",
    "\n",
    "# select the columns based of the logics \n",
    "df_pyspark.select([str(v) for v in df_pyspark.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get the datafram row\n",
    "print(df_pyspark.collect()[0])\n",
    "\n",
    "# get particulat element in particular row \n",
    "print(df_pyspark.collect()[0]['Age'])# or \n",
    "print(df_pyspark.collect()[0][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+------------------------+\n",
      "|    Name|Age|Expricnce|Expricen after two years|\n",
      "+--------+---+---------+------------------------+\n",
      "|   vicky| 21|        1|                       3|\n",
      "|  rajeev| 22|        3|                       5|\n",
      "|   mohan| 21|        4|                       6|\n",
      "|    modi|  3|        5|                       7|\n",
      "| shubham|  5|        8|                      10|\n",
      "|rajeever|  4|       44|                      46|\n",
      "|  preeti| 21|        2|                       4|\n",
      "+--------+---+---------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding a column to a dataframe \n",
    "\n",
    "# both works int the same way df['column_name'] gives only column and we can apply any operations on that but we can not perform show or collect on that for that we\n",
    "# have to use select or show method \n",
    "\n",
    "\n",
    "\n",
    "# thsese are not inplace operations we have to assign to another dataframe \n",
    "# df_pyspark.withColumn('Exprience after two years', df_pyspark.Expricnce + 2).show()\n",
    "df_pyspark.withColumn('Expricen after two years', df_pyspark['Expricnce'] + 2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+\n",
      "|    Name|Age|Expricnce|\n",
      "+--------+---+---------+\n",
      "|   vicky| 21|        1|\n",
      "|  rajeev| 22|        3|\n",
      "|   mohan| 21|        4|\n",
      "|    modi|  3|        5|\n",
      "| shubham|  5|        8|\n",
      "|rajeever|  4|       44|\n",
      "|  preeti| 21|        2|\n",
      "+--------+---+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|    Name|Expricnce|\n",
      "+--------+---------+\n",
      "|   vicky|        1|\n",
      "|  rajeev|        3|\n",
      "|   mohan|        4|\n",
      "|    modi|        5|\n",
      "| shubham|        8|\n",
      "|rajeever|       44|\n",
      "|  preeti|        2|\n",
      "+--------+---------+\n",
      "\n",
      "+---+--------+\n",
      "|Age|    Name|\n",
      "+---+--------+\n",
      "| 21|   vicky|\n",
      "| 22|  rajeev|\n",
      "| 21|   mohan|\n",
      "|  3|    modi|\n",
      "|  5| shubham|\n",
      "|  4|rajeever|\n",
      "| 21|  preeti|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the column from the dataframe \n",
    "df_pyspark = df_pyspark.drop('Expricen after two year').show()\n",
    "\n",
    "\n",
    "# drop multiple columns\n",
    "df_pyspark.drop('Age','Expricen after two year').show()\n",
    "# df_pyspark.select(['Age','Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---------+\n",
      "|New Name|Age|Expricnce|\n",
      "+--------+---+---------+\n",
      "|   vicky| 21|        1|\n",
      "|  rajeev| 22|        3|\n",
      "|   mohan| 21|        4|\n",
      "|    modi|  3|        5|\n",
      "| shubham|  5|        8|\n",
      "|rajeever|  4|       44|\n",
      "|  preeti| 21|        2|\n",
      "+--------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename the column in the dataframe \n",
    "df_pyspark.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd tutorial \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 Droping Rows\n",
    "2 Droping Columns\n",
    "3 various parameters in droping funcnility \n",
    "4 handling missing values by mean,median and mode \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+------+\n",
      "|  Name | Age|Exprience|Salary|\n",
      "+-------+----+---------+------+\n",
      "| Vicky |  21|        1| 17000|\n",
      "| Rajeev|  22|        2| 42000|\n",
      "| Pandit|  23|        1| 56000|\n",
      "|shubham|  21|        2| 28000|\n",
      "|   Modi|  22|        3| 75000|\n",
      "|  Sunny|  28|        4| 38000|\n",
      "| mahesh|null|     null| 45000|\n",
      "|   null|  34|       10| 38000|\n",
      "|   null|  36|     null|  null|\n",
      "+-------+----+---------+------+\n",
      "\n",
      "root\n",
      " |-- Name : string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Exprience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('/home/vkkr125/programming/bigdata/test3.csv',header = True,inferSchema = True)\n",
    "df_pyspark.show()\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|  Name | Age|Salary|\n",
      "+-------+----+------+\n",
      "| Vicky |  21| 17000|\n",
      "| Rajeev|  22| 42000|\n",
      "| Pandit|  23| 56000|\n",
      "|shubham|  21| 28000|\n",
      "|   Modi|  22| 75000|\n",
      "|  Sunny|  28| 38000|\n",
      "| mahesh|null| 45000|\n",
      "|   null|  34| 38000|\n",
      "|   null|  36|  null|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_pyspark.drop('Age').show()\n",
    "df_pyspark.drop('Exprience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+------+\n",
      "|  Name |Age|Exprience|Salary|\n",
      "+-------+---+---------+------+\n",
      "| Vicky | 21|        1| 17000|\n",
      "| Rajeev| 22|        2| 42000|\n",
      "| Pandit| 23|        1| 56000|\n",
      "|shubham| 21|        2| 28000|\n",
      "|   Modi| 22|        3| 75000|\n",
      "|  Sunny| 28|        4| 38000|\n",
      "+-------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all the rows will be deleted where atleas one column has null value\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+------+\n",
      "|  Name | Age|Exprience|Salary|\n",
      "+-------+----+---------+------+\n",
      "| Vicky |  21|        1| 17000|\n",
      "| Rajeev|  22|        2| 42000|\n",
      "| Pandit|  23|        1| 56000|\n",
      "|shubham|  21|        2| 28000|\n",
      "|   Modi|  22|        3| 75000|\n",
      "|  Sunny|  28|        4| 38000|\n",
      "| mahesh|null|     null| 45000|\n",
      "|   null|  34|       10| 38000|\n",
      "|   null|  36|     null|  null|\n",
      "+-------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# those rows will be deleted which row has all the columns vlues are null\n",
    "df_pyspark.na.drop(how = 'all').show()\n",
    "# we don't have any column that has all the values null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+------+\n",
      "|  Name | Age|Exprience|Salary|\n",
      "+-------+----+---------+------+\n",
      "| Vicky |  21|        1| 17000|\n",
      "| Rajeev|  22|        2| 42000|\n",
      "| Pandit|  23|        1| 56000|\n",
      "|shubham|  21|        2| 28000|\n",
      "|   Modi|  22|        3| 75000|\n",
      "|  Sunny|  28|        4| 38000|\n",
      "| mahesh|null|     null| 45000|\n",
      "|   null|  34|       10| 38000|\n",
      "+-------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# all the rows will be deleted that has not more that 2 not null values\n",
    "# we can provide the threshold value to the function to drop the na values\n",
    "df_pyspark.na.drop(thresh = 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+------+\n",
      "|  Name | Age|Exprience|Salary|\n",
      "+-------+----+---------+------+\n",
      "| Vicky |  21|        1| 17000|\n",
      "| Rajeev|  22|        2| 42000|\n",
      "| Pandit|  23|        1| 56000|\n",
      "|shubham|  21|        2| 28000|\n",
      "|   Modi|  22|        3| 75000|\n",
      "|  Sunny|  28|        4| 38000|\n",
      "| mahesh|null|     null| 45000|\n",
      "+-------+----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all the rows will be deteted where name is null \n",
    "df_pyspark.na.drop(subset = ['Name ']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+------+\n",
      "|  Name | Age|Exprience|Salary|\n",
      "+-------+----+---------+------+\n",
      "| Vicky |  21|        1| 17000|\n",
      "| Rajeev|  22|        2| 42000|\n",
      "| Pandit|  23|        1| 56000|\n",
      "|shubham|  21|        2| 28000|\n",
      "|   Modi|  22|        3| 75000|\n",
      "|  Sunny|  28|        4| 38000|\n",
      "| mahesh|null|     null| 45000|\n",
      "|missing|  34|       10| 38000|\n",
      "|missing|  36|     null|  null|\n",
      "+-------+----+---------+------+\n",
      "\n",
      "+-------+---+---------+------+\n",
      "|  Name |Age|Exprience|Salary|\n",
      "+-------+---+---------+------+\n",
      "| Vicky | 21|        1| 17000|\n",
      "| Rajeev| 22|        2| 42000|\n",
      "| Pandit| 23|        1| 56000|\n",
      "|shubham| 21|        2| 28000|\n",
      "|   Modi| 22|        3| 75000|\n",
      "|  Sunny| 28|        4| 38000|\n",
      "| mahesh| 20|       20| 45000|\n",
      "|   null| 34|       10| 38000|\n",
      "|   null| 36|       20|    20|\n",
      "+-------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill the missing values \n",
    "\n",
    "# null vlues in name column is now missing\n",
    "df_pyspark.na.fill(value = 'missing',subset = ['Name ']).show()\n",
    "df_pyspark.na.fill(value = 20).show() # all the null will be filled with 20 no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+----------+\n",
      "|  Name | Age|Exprience|New Salary|\n",
      "+-------+----+---------+----------+\n",
      "| Vicky |  21|        1|     34000|\n",
      "| Rajeev|  22|        2|     84000|\n",
      "| Pandit|  23|        1|    112000|\n",
      "|shubham|  21|        2|     56000|\n",
      "|   Modi|  22|        3|    150000|\n",
      "|  Sunny|  28|        4|     76000|\n",
      "| mahesh|null|     null|     90000|\n",
      "|   null|  34|       10|     76000|\n",
      "|   null|  36|     null|      null|\n",
      "+-------+----+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# multiply the salary column by 2 \n",
    "df_pyspark.withColumn('New Salary',df_pyspark['Salary'] * 2).drop('Salary').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3rd tutorial"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filter operation\n",
    "&,|,==\n",
    "~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+------+\n",
      "|  Name |Age|Exprience|Salary|\n",
      "+-------+---+---------+------+\n",
      "| Vicky | 21|        1| 17000|\n",
      "| Rajeev| 22|        2| 42000|\n",
      "| Pandit| 23|        1| 56000|\n",
      "|shubham| 21|        2| 28000|\n",
      "|   Modi| 22|        3| 75000|\n",
      "|  Sunny| 28|        4| 38000|\n",
      "+-------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('/home/vkkr125/programming/bigdata/test3.csv',header = True,inferSchema = True).na.drop()\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter operations on pyspark dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+------+\n",
      "|  Name |Age|Exprience|Salary|\n",
      "+-------+---+---------+------+\n",
      "| Vicky | 21|        1| 17000|\n",
      "|shubham| 21|        2| 28000|\n",
      "|  Sunny| 28|        4| 38000|\n",
      "+-------+---+---------+------+\n",
      "\n",
      "+-------+---+---------+------+\n",
      "|  Name |Age|Exprience|Salary|\n",
      "+-------+---+---------+------+\n",
      "| Vicky | 21|        1| 17000|\n",
      "|shubham| 21|        2| 28000|\n",
      "|  Sunny| 28|        4| 38000|\n",
      "+-------+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('Salary <= 40000').show()\n",
    "df_pyspark.filter(df_pyspark['Salary'] <= 40000).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---------+------+\n",
      "| Name |Age|Exprience|Salary|\n",
      "+------+---+---------+------+\n",
      "|Rajeev| 22|        2| 42000|\n",
      "|Pandit| 23|        1| 56000|\n",
      "| Sunny| 28|        4| 38000|\n",
      "+------+---+---------+------+\n",
      "\n",
      "+------+---------+\n",
      "| Name |Exprience|\n",
      "+------+---------+\n",
      "|Rajeev|        2|\n",
      "|Pandit|        1|\n",
      "| Sunny|        4|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rows with salary <= 60000 and >= 300000\n",
    "df_pyspark.filter((df_pyspark['Salary'] <= 60000) & (df_pyspark['Salary'] >= 30000)).show()\n",
    "\n",
    "# take only 2 rows \n",
    "df_pyspark.filter((df_pyspark['Salary'] <= 60000) & (df_pyspark['Salary'] >= 30000)).select('Name ','Exprience').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "| Name |Exprience|\n",
      "+------+---------+\n",
      "|Rajeev|        2|\n",
      "|Pandit|        1|\n",
      "| Sunny|        4|\n",
      "+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary'] <= 60000) & (df_pyspark['Salary'] >= 30000)).select('Name ','Exprience').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4th tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('/home/vkkr125/programming/bigdata/test4.csv',header = True,inferSchema = True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n",
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mean salary of each departmenst \n",
    "df_pyspark.groupBy('Departments').mean().show() \n",
    "df_pyspark.groupBy('Departments').count().show() # number of people on each departments \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can directely apply aggrigate functions\n",
    "# total salary /expendature \n",
    "df_pyspark.agg({'Salary' : 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init()\n",
    "import py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('practice2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.189.227:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f933c6eb8e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark.sparkContext.textFile('/home/vkkr125/programming/bigdata/u.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['196 211 3 883848348',\n",
       " '323 32 3 848948924',\n",
       " '311 22 1 824839853',\n",
       " '22 3777 2 84839589',\n",
       " '111 844 1 84394389']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('/home/vkkr125/Downloads/yellow_tripdata_2018-01.csv',header = True,inferSchema = True)\n",
    "# df_pyspark.show(10)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# number of columns\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8759874"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows in dataframe\n",
    "\n",
    "df_pyspark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|(total_amount >= 10)|\n",
      "+--------------------+\n",
      "|               false|\n",
      "|                true|\n",
      "|               false|\n",
      "|                true|\n",
      "|                true|\n",
      "|               false|\n",
      "|                true|\n",
      "|               false|\n",
      "|               false|\n",
      "|               false|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(df_pyspark['total_amount'] >=10).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8759874"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_pyspark.na.drop().count()\n",
    "\n",
    "#count of data after droping the null values , same as previous \n",
    "# so the data is cleaned there is no null values in dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Query 1. - Add a column named as \"Revenue\" into dataframe which is the sum of the below columns 'Fare_amount','Extra','MTA_tax','Improvement_surcharge','Tip_amount','Tolls_amount','Total_amount'\n",
    "Query 2. - Increasing count of total passengers in New York City by area\n",
    "Query 3. - Realtime Average fare/total earning amount earned by 2 vendors\n",
    "Query 4. - Moving Count of payments made by each payment mode\n",
    "Query 5. - Highest two gaining vendor's on a particular date with no of passenger and total distance by cab\n",
    "Query 6. - Most no of passenger between a route of two location.\n",
    "Query 7. - Get top pickup locations with most passengers in last 5/10 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           Revenue|\n",
      "+------------------+\n",
      "|              11.6|\n",
      "|              30.6|\n",
      "|              16.6|\n",
      "|              69.6|\n",
      "|              33.1|\n",
      "|              11.6|\n",
      "|24.700000000000003|\n",
      "|              12.6|\n",
      "|              17.0|\n",
      "|              13.6|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 1. - Add a column named as \"Revenue\" into dataframe which is the sum of the below columns 'Fare_amount','Extra','MTA_tax','Improvement_surcharge','Tip_amount','Tolls_amount','Total_amount'\n",
    "\n",
    "df_pyspark = df_pyspark.withColumn('Revenue',(df_pyspark['fare_amount'] + df_pyspark['extra'] + df_pyspark['mta_tax'] \\\n",
    "                      + df_pyspark['improvement_surcharge'] + df_pyspark['tip_amount'] + df_pyspark['tolls_amount'] \\\n",
    "                      + df_pyspark['total_amount']))\n",
    "df_pyspark.select('Revenue').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'Revenue']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of columns now\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n",
      "|PULocationID|total_passengers|\n",
      "+------------+----------------+\n",
      "|         204|               1|\n",
      "|          44|               1|\n",
      "|         184|               2|\n",
      "|         187|               2|\n",
      "|           5|               3|\n",
      "|          46|               3|\n",
      "|         176|               3|\n",
      "|         199|               4|\n",
      "|          58|               4|\n",
      "|           2|               4|\n",
      "|          30|               4|\n",
      "|         245|               5|\n",
      "|         221|               6|\n",
      "|         214|               6|\n",
      "|         156|               8|\n",
      "|         105|               8|\n",
      "|         109|               8|\n",
      "|          84|               8|\n",
      "|         251|               9|\n",
      "|          27|              12|\n",
      "+------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query 2. - Increasing count of total passengers in New York City by area\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "df_pyspark.groupBy('PULocationID').agg(f.sum('passenger_count').alias('total_passengers')) \\\n",
    "          .sort('total_passengers').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|PULocationID|total_passenger|\n",
      "+------------+---------------+\n",
      "|         204|              1|\n",
      "|          44|              1|\n",
      "|         184|              2|\n",
      "|         187|              2|\n",
      "|         176|              3|\n",
      "|          46|              3|\n",
      "|           5|              3|\n",
      "|         199|              4|\n",
      "|          58|              4|\n",
      "|           2|              4|\n",
      "+------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# above query using sql \n",
    "spark.sql('select PULocationID,sum(passenger_count) as total_passenger from bda100_table group by PULocationID order by total_passenger').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|VendorID|\n",
      "+--------+\n",
      "|       1|\n",
      "|       2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 3. - Realtime Average fare/total earning amount earned by 2 vendors\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "# total number of vendors \n",
    "\n",
    "\n",
    "df_pyspark.select('VendorID').distinct().show()\n",
    "\n",
    "total_fare_amount = df_pyspark.select('fare_amount').agg(f.sum('fare_amount').alias('total_fare_amount'))\n",
    "total_amount = df_pyspark.select('total_amount').agg(f.sum('total_amount').alias('total_amount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   total_fare_amount|\n",
      "+--------------------+\n",
      "|1.0725890238000062E8|\n",
      "+--------------------+\n",
      "\n",
      "+------------------+\n",
      "|      total_amount|\n",
      "+------------------+\n",
      "|1.35699048741755E8|\n",
      "+------------------+\n",
      "\n",
      "Average  0.7904174964713421\n"
     ]
    }
   ],
   "source": [
    "total_fare_amount.show()\n",
    "total_amount.show()\n",
    "x = total_fare_amount.collect()\n",
    "y = total_amount.collect()\n",
    "\n",
    "# average of [fare / total]\n",
    "print('Average ', x[0].total_fare_amount / y[0].total_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|payment_type|       total_amount|\n",
      "+------------+-------------------+\n",
      "|           1|2.046995811610078E8|\n",
      "|           3| 1084041.3600001158|\n",
      "|           4| 294450.68999999244|\n",
      "|           2|6.528970845947792E7|\n",
      "+------------+-------------------+\n",
      "\n",
      "+------------+----------+\n",
      "|payment_type|tota_count|\n",
      "+------------+----------+\n",
      "|           1|   6105871|\n",
      "|           3|     43204|\n",
      "|           4|     11852|\n",
      "|           2|   2598947|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 4. - Moving Count of payments made by each payment mode\n",
    "\n",
    "\n",
    "# total payment_made by each payment mode\n",
    "df_pyspark.groupBy('payment_type').agg(f.sum('Revenue').alias('total_amount')).show()\n",
    "\n",
    "\n",
    "# total count of payment made by each payment mode \n",
    "df_pyspark.groupBy('payment_type').agg(f.count('Revenue').alias('tota_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+\n",
      "|payment_type|      total_revenue|\n",
      "+------------+-------------------+\n",
      "|           1|2.046995811610078E8|\n",
      "|           3| 1084041.3600001158|\n",
      "|           4| 294450.68999999244|\n",
      "|           2|6.528970845947792E7|\n",
      "+------------+-------------------+\n",
      "\n",
      "+------------+-----------+\n",
      "|payment_type|total_count|\n",
      "+------------+-----------+\n",
      "|           1|    6105871|\n",
      "|           3|      43204|\n",
      "|           4|      11852|\n",
      "|           2|    2598947|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# above query usign spark sql \n",
    "\n",
    "df_pyspark.createOrReplaceTempView(\"bda100_table\")\n",
    "spark.sql('select payment_type,sum(Revenue) as total_revenue from bda100_table group by payment_type').show()\n",
    "\n",
    "spark.sql('select payment_type,count(Revenue) as total_count from bda100_table group by payment_type').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------------+-----------------+\n",
      "|      date|          total_gain|total_passengers|   total_distance|\n",
      "+----------+--------------------+----------------+-----------------+\n",
      "|2018-01-25|1.0550837729971118E7|          523815|902081.6699999892|\n",
      "|2018-01-26|1.0548607099968836E7|          536344|901778.8499999947|\n",
      "+----------+--------------------+----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 5. - Highest two gaining vendor's on a particular date with no of passenger and total distance by cab\n",
    "\n",
    "df_pyspark.groupBy(f.to_date('tpep_pickup_datetime').alias('date')).agg(\n",
    "                  f.sum('Revenue').alias('total_gain'),\n",
    "                  f.sum('passenger_count').alias('total_passengers'),\n",
    "                  f.sum('trip_distance').alias('total_distance')\n",
    ").orderBy(f.sum('Revenue').desc()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------------+\n",
      "|PULocationID|DOLocationID|total_passengers|\n",
      "+------------+------------+----------------+\n",
      "|         114|         257|             171|\n",
      "|          83|         142|               7|\n",
      "|         229|         239|            3211|\n",
      "|          25|          61|             401|\n",
      "|         148|         229|            1995|\n",
      "|         107|         161|           10588|\n",
      "|         264|         107|             537|\n",
      "|         151|         116|            1579|\n",
      "|         231|          41|             380|\n",
      "|         141|         173|             101|\n",
      "+------------+------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 6. - Most no of passenger between a route of two location.\n",
    "\n",
    "df_pyspark.groupBy('PULocationID','DOLocationID').agg(f.sum('passenger_count').alias('total_passengers')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------------+\n",
      "|PULocationID|DOLocationID|total_passengers|\n",
      "+------------+------------+----------------+\n",
      "|         114|         257|             171|\n",
      "|          83|         142|               7|\n",
      "|         229|         239|            3211|\n",
      "|          25|          61|             401|\n",
      "|         148|         229|            1995|\n",
      "|         107|         161|           10588|\n",
      "|         264|         107|             537|\n",
      "|         151|         116|            1579|\n",
      "|         231|          41|             380|\n",
      "|         141|         173|             101|\n",
      "+------------+------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# above query using sql\n",
    "spark.sql('select PULocationID,DOLocationID,sum(passenger_count) as total_passengers \\\n",
    "          from bda100_table group by PULocationID,DOLocationID').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top pickup locations with most passengers in last 5/10 seconds\n",
    "# tpep_pickup_datetime + next 10 minutes \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
